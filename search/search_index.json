{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Description This pipeline is based on snakemake and the dropseq tools provided by the McCarroll Lab . It allows to go from raw data of your Single Cell RNA seq experiment until the final count matrix with QC plots along the way. This is the tool we use in our lab to improve our wetlab protocol as well as provide an easy framework to reproduce and compare different experiments with different parameters. It uses STAR to map the reads. It is usable for any single cell protocol using two reads where the first one holds the Cell and UMI barcodes and the second read holds the RNA. Here is a non-exhausitve list of compatible protocols: Drop-Seq SCRB-Seq 10x Genomics DroNc-seq This package is trying to be as user friendly as possible. One of the hopes is that non-bioinformatician can make use of it without too much hassle. It will still require some command line execution, this is not going to be fully interactive package. Latest changes [0.31] Changed Fixed error for STAR index generation. It crashed saying it couldn't write in folder. Fixed a missing plot for plot_knee_plot_whitelist. Input files for the STAR_align rule have been changed. Adding samples in an already aligned experiment with a different R2 length, will only align the new data and not realign the old one. Split reads and barcodes multiqc reports for qc step. Modified a few rules to follow the guidelines for snakemake workflows Fixed an issue where snakemake would crash on clusters if using expand() on fixed variables such as annotation_prefix . Now using normal python formatting. Changed the config.yaml parameters names to lowercase and hyphens! Software specific variables have their original style making it easier to search in manuals. You will have to either copy the new config.yaml from the templates or modify your own accordingly. cell-barcode-edit-distance changed to what it actually is, UMI-edit-distance. Updated all the envs to fix bugs. Fixed a bug where the mixed species would not run properly. Added Added ggpubr in environment.yaml file. Added a templates folder which will hold config.yaml , samples.csv , cluster.yaml as well as adapters files. This will also help cloning the repository without overwritting your own config.yaml file when updating the pipeline. Added the possibility of using your own adapters fasta file for trimmomatic. To use it, please refer to the WIKI Added fastqc, multiqc, STAR wrappers. You have now to use the --use-conda option to run the pipeline. Added cluster recommendations on the wiki. Added Localrules for certain rules. This allows to run low ressource rules on the host computer instead of nodes when using clusters. genomeChrBinNbits will be calculated automacially for STAR. Exposed all variables for trimmomatic in config.yaml under trimming. Removed png plots have been removed. It was causing some issues on clusters with cairo. Usability is more important than png plots to me. installation The installation process can be found in the Installation section of the wiki. Future implementations I'm actively seeking help to implement the points listed bellow. Don't hesitate to contact me if you wish to contribute. Create a sharing platform where quality plots/logs can be discussed and troubleshooted. Create a full html report for the whole pipeline Multiqc module for drop-seq-tools Conda package for drop-seq-tools RData object of all the summary data and plots so that you can create your own report. Implement an elegant \"preview\" mode where the pipeline would only run on a couple of millions of reads and allow you to have an approximated view before running all of the data. This would dramatically reduce the time needed to get an idea of what filters whould be used. I hope it can help you out in your single cell experiments! Feel free to comment and point out potential improvements.","title":"Home"},{"location":"#description","text":"This pipeline is based on snakemake and the dropseq tools provided by the McCarroll Lab . It allows to go from raw data of your Single Cell RNA seq experiment until the final count matrix with QC plots along the way. This is the tool we use in our lab to improve our wetlab protocol as well as provide an easy framework to reproduce and compare different experiments with different parameters. It uses STAR to map the reads. It is usable for any single cell protocol using two reads where the first one holds the Cell and UMI barcodes and the second read holds the RNA. Here is a non-exhausitve list of compatible protocols: Drop-Seq SCRB-Seq 10x Genomics DroNc-seq This package is trying to be as user friendly as possible. One of the hopes is that non-bioinformatician can make use of it without too much hassle. It will still require some command line execution, this is not going to be fully interactive package.","title":"Description"},{"location":"#latest-changes","text":"","title":"Latest changes"},{"location":"#031","text":"","title":"[0.31]"},{"location":"#changed","text":"Fixed error for STAR index generation. It crashed saying it couldn't write in folder. Fixed a missing plot for plot_knee_plot_whitelist. Input files for the STAR_align rule have been changed. Adding samples in an already aligned experiment with a different R2 length, will only align the new data and not realign the old one. Split reads and barcodes multiqc reports for qc step. Modified a few rules to follow the guidelines for snakemake workflows Fixed an issue where snakemake would crash on clusters if using expand() on fixed variables such as annotation_prefix . Now using normal python formatting. Changed the config.yaml parameters names to lowercase and hyphens! Software specific variables have their original style making it easier to search in manuals. You will have to either copy the new config.yaml from the templates or modify your own accordingly. cell-barcode-edit-distance changed to what it actually is, UMI-edit-distance. Updated all the envs to fix bugs. Fixed a bug where the mixed species would not run properly.","title":"Changed"},{"location":"#added","text":"Added ggpubr in environment.yaml file. Added a templates folder which will hold config.yaml , samples.csv , cluster.yaml as well as adapters files. This will also help cloning the repository without overwritting your own config.yaml file when updating the pipeline. Added the possibility of using your own adapters fasta file for trimmomatic. To use it, please refer to the WIKI Added fastqc, multiqc, STAR wrappers. You have now to use the --use-conda option to run the pipeline. Added cluster recommendations on the wiki. Added Localrules for certain rules. This allows to run low ressource rules on the host computer instead of nodes when using clusters. genomeChrBinNbits will be calculated automacially for STAR. Exposed all variables for trimmomatic in config.yaml under trimming.","title":"Added"},{"location":"#removed","text":"png plots have been removed. It was causing some issues on clusters with cairo. Usability is more important than png plots to me.","title":"Removed"},{"location":"#installation","text":"The installation process can be found in the Installation section of the wiki.","title":"installation"},{"location":"#future-implementations","text":"I'm actively seeking help to implement the points listed bellow. Don't hesitate to contact me if you wish to contribute. Create a sharing platform where quality plots/logs can be discussed and troubleshooted. Create a full html report for the whole pipeline Multiqc module for drop-seq-tools Conda package for drop-seq-tools RData object of all the summary data and plots so that you can create your own report. Implement an elegant \"preview\" mode where the pipeline would only run on a couple of millions of reads and allow you to have an approximated view before running all of the data. This would dramatically reduce the time needed to get an idea of what filters whould be used. I hope it can help you out in your single cell experiments! Feel free to comment and point out potential improvements.","title":"Future implementations"},{"location":"CHANGELOG/","text":"Change Log All notable changes to this project will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning . [0.31a] Changed fix on species plot. fix on rule STAR_align adding now unmapped read to a fastq file. Added Added travis integration. The pipeline is now automatically getting tested when updated and when pull requests are proposed. There is now a small git submodule in .test which will provide a sampled file for testing the pipeline on travis-ci. Removed environment.yaml has been removed. Youjust have to install snakemake now instead of activating the env. [0.31] Changed Fixed error for STAR index generation. It crashed saying it couldn't write in folder. Fixed a missing plot for plot_knee_plot_whitelist. Input files for the STAR_align rule have been changed. Adding samples in an already aligned experiment with a different R2 length, will only align the new data and not realign the old one. Split reads and barcodes multiqc reports for qc step. Modified a few rules to follow the guidelines for snakemake workflows Fixed an issue where snakemake would crash on clusters if using expand() on fixed variables such as annotation_prefix . Now using normal python formatting. Changed the config.yaml parameters names to lowercase and hyphens! Software specific variables have their original style making it easier to search in manuals. You will have to either copy the new config.yaml from the templates or modify your own accordingly. cell-barcode-edit-distance changed to what it actually is, UMI-edit-distance. Updated all the envs to fix bugs. Fixed a bug where the mixed species would not run properly. Added Added ggpubr in environment.yaml file. Added a templates folder which will hold config.yaml , samples.csv , cluster.yaml as well as adapters files. This will also help cloning the repository without overwritting your own config.yaml file when updating the pipeline. Added the possibility of using your own adapters fasta file for trimmomatic. To use it, please refer to the WIKI Added fastqc, multiqc, STAR wrappers. You have now to use the --use-conda option to run the pipeline. Added cluster recommendations on the wiki. Added Localrules for certain rules. This allows to run low ressource rules on the host computer instead of nodes when using clusters. genomeChrBinNbits will be calculated automacially for STAR. Exposed all variables for trimmomatic in config.yaml under trimming. Removed png plots have been removed. It was causing some issues on clusters with cairo. Usability is more important than png plots to me. [0.3] Changed Complete overhaul of how the pipeline is organized to follow the structure proposed for snakemake-workflows. This will allow ease of deployement on any platform having conda installed. It will also help to run on clusters. The way to call the pipeline is now simplified. Changes are shown in the WIKI Dependency to Drop-seq-tools updated from version 1.12 to 1.13 Full compatibility with barcode whitelist. Makes it easier to use for SCRBseq protocols or whitelist from other source (UMI-tools). Modified cell and UMI drop plots in order to reflect the option chosen. See plots Removed Bulk sequencing compatiblity. Fastqc and STAR logs plots are removed and replaced by multiqc. Automatic determination of STAMPS via knee_plot. Please use an estimated number of cells as the main threshold and filter in downstream analysis for other parameters such as high number of mitochondrial genes. MinCellFraction entry in config.yaml. This parameter wasn't adding much value and was confusing. Base frequency plot has been removed. This will come back with autodetermination of the STAMPS. Added Wrapper for Drop-seq tools. Makes it easier to switch temp folder and choose maximum memory heap. More parameters for STAR exposed. See WIKI [0.24] Changed All the QCplots are now generated inside the snakefiles. No more generate-plots mode. [0.23a] Changed Will now allow you to run generate-meta without having a config.yaml file in the reference foder. Changed the code for Cell and UMI barcode quality drop (per sample and overall). There was an error in the code not givint the right amount of dropped reads. Updated the images on the wiki accordingly. Fixed the setup where r2py was called before getting installed. Big change in the mapping. From now on the STAR index will be done without a GTF file. This allows to change the overhang option on the fly for each sample based on the mean read length. This also opens up 2-pass mapping. You will have to regenerate your index for it to work. Changed generate_meta in order to fit the new STAR index without a GTF. You now have to give the path to the GTF file in the config.yaml Added min_count_per_umi in the config.yaml to decide how many times a Gene - UMI has to be found to be counted as one. [0.23] Changed pre_align steps will output a fastq.gz instead of a fastq file. fastqc.R is now compatible with paired and single end data. Changed a few options in GLOBAL for UMI and Cell_barcodes options. Now possible to change filtering settings. See WIKI STAR logs have been stripped of the STAR string. This is to allow for better compatibility with multiqc Removed fastqc folder and moved items to logs folder. Grouping all logs files for better multiqc compatibility. Changed generate_meta to generate-meta for keeping similar syntax between modes. Added seperate log files for stats and summary in the DetectBeadSynthesisErrors. Moved part of the README to the wiki. Changed the name of the first expression matrix extracted before the species plot to unfiltered_expression. Added You can now run Bulk Single or paired end RNAseq data. Started a wiki with a FAQ Added options in GLOBAL config.yaml. You can now choose a range of options for UMI and Barcode filtering. please refer to the wiki for more information. Support for MultiQC . MultiQC is a great way of summarising all of the logs from your experiment. As of today it supports 46 different modules (such as fastqc, trimmomatic, STAR, etc...) The generate-plots mode now produces a multiqc_report.html file in the plots folder. New plot! BCDrop.pdf is a new plot showing you how many barcode and UMIs you dropped from the raw data before aligning. This helps to track how many samples you might loose because of low quality reads in the barcoding. [0.22] Changed all subprocess.call replaced by shell from snakemake STAR aligner now not limited to 8 cores or threads but will use the maximum number provided in the local.yaml file Name from dropSeqPip to dropSeqPipe Fixed a bug where all stage1 steps used the same summary file. Now BC tagging, UMI tagging, starting trim and polyA trim have different summary files extract-expression now merges all the samples final count matrix into one per run (folder) Fixed a bug where the amount of total reads on the knee-plot was overinflated. Changed knee-plot mode to generate-plots . Added Temp files have been added in the pipeline. You can turn this off by using the --notemp option fastqc mode now available. Generates fastqc reports plus summary plots Summary file and plot for fastqc and STAR logs Missing R packages should install automatically now. No need to install them beforehand. Report any problem plz GLOBAL values in the config files are now available. They allow to change UMI and BC ranges as well as mismatches for STAR aligner Added a new mode: generate_meta. This allows to create all the metadata files needed for the pipeline. You just need a folder with a genome.fa and an annotation.gtf [0.21] Added Changelog file to track changes --rerun option to force a rerun Multiple steps allowd now [0.2] - 2017-03-14 Changed The pipeline is now a python package being called as an executable Went from json to yaml for config files Added setup.py and dependencies Species plot available Removed primer handling, went to default: AAGCAGTGGTATCAACGCAGAGTAC [0.1] - 2017-02-13 First release Allows for preprocessing, alignement with STAR, post align processing until knee-plot","title":"Change Log"},{"location":"CHANGELOG/#change-log","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning .","title":"Change Log"},{"location":"CHANGELOG/#031a","text":"","title":"[0.31a]"},{"location":"CHANGELOG/#changed","text":"fix on species plot. fix on rule STAR_align adding now unmapped read to a fastq file.","title":"Changed"},{"location":"CHANGELOG/#added","text":"Added travis integration. The pipeline is now automatically getting tested when updated and when pull requests are proposed. There is now a small git submodule in .test which will provide a sampled file for testing the pipeline on travis-ci.","title":"Added"},{"location":"CHANGELOG/#removed","text":"environment.yaml has been removed. Youjust have to install snakemake now instead of activating the env.","title":"Removed"},{"location":"CHANGELOG/#031","text":"","title":"[0.31]"},{"location":"CHANGELOG/#changed_1","text":"Fixed error for STAR index generation. It crashed saying it couldn't write in folder. Fixed a missing plot for plot_knee_plot_whitelist. Input files for the STAR_align rule have been changed. Adding samples in an already aligned experiment with a different R2 length, will only align the new data and not realign the old one. Split reads and barcodes multiqc reports for qc step. Modified a few rules to follow the guidelines for snakemake workflows Fixed an issue where snakemake would crash on clusters if using expand() on fixed variables such as annotation_prefix . Now using normal python formatting. Changed the config.yaml parameters names to lowercase and hyphens! Software specific variables have their original style making it easier to search in manuals. You will have to either copy the new config.yaml from the templates or modify your own accordingly. cell-barcode-edit-distance changed to what it actually is, UMI-edit-distance. Updated all the envs to fix bugs. Fixed a bug where the mixed species would not run properly.","title":"Changed"},{"location":"CHANGELOG/#added_1","text":"Added ggpubr in environment.yaml file. Added a templates folder which will hold config.yaml , samples.csv , cluster.yaml as well as adapters files. This will also help cloning the repository without overwritting your own config.yaml file when updating the pipeline. Added the possibility of using your own adapters fasta file for trimmomatic. To use it, please refer to the WIKI Added fastqc, multiqc, STAR wrappers. You have now to use the --use-conda option to run the pipeline. Added cluster recommendations on the wiki. Added Localrules for certain rules. This allows to run low ressource rules on the host computer instead of nodes when using clusters. genomeChrBinNbits will be calculated automacially for STAR. Exposed all variables for trimmomatic in config.yaml under trimming.","title":"Added"},{"location":"CHANGELOG/#removed_1","text":"png plots have been removed. It was causing some issues on clusters with cairo. Usability is more important than png plots to me.","title":"Removed"},{"location":"CHANGELOG/#03","text":"","title":"[0.3]"},{"location":"CHANGELOG/#changed_2","text":"Complete overhaul of how the pipeline is organized to follow the structure proposed for snakemake-workflows. This will allow ease of deployement on any platform having conda installed. It will also help to run on clusters. The way to call the pipeline is now simplified. Changes are shown in the WIKI Dependency to Drop-seq-tools updated from version 1.12 to 1.13 Full compatibility with barcode whitelist. Makes it easier to use for SCRBseq protocols or whitelist from other source (UMI-tools). Modified cell and UMI drop plots in order to reflect the option chosen. See plots","title":"Changed"},{"location":"CHANGELOG/#removed_2","text":"Bulk sequencing compatiblity. Fastqc and STAR logs plots are removed and replaced by multiqc. Automatic determination of STAMPS via knee_plot. Please use an estimated number of cells as the main threshold and filter in downstream analysis for other parameters such as high number of mitochondrial genes. MinCellFraction entry in config.yaml. This parameter wasn't adding much value and was confusing. Base frequency plot has been removed. This will come back with autodetermination of the STAMPS.","title":"Removed"},{"location":"CHANGELOG/#added_2","text":"Wrapper for Drop-seq tools. Makes it easier to switch temp folder and choose maximum memory heap. More parameters for STAR exposed. See WIKI","title":"Added"},{"location":"CHANGELOG/#024","text":"","title":"[0.24]"},{"location":"CHANGELOG/#changed_3","text":"All the QCplots are now generated inside the snakefiles. No more generate-plots mode.","title":"Changed"},{"location":"CHANGELOG/#023a","text":"","title":"[0.23a]"},{"location":"CHANGELOG/#changed_4","text":"Will now allow you to run generate-meta without having a config.yaml file in the reference foder. Changed the code for Cell and UMI barcode quality drop (per sample and overall). There was an error in the code not givint the right amount of dropped reads. Updated the images on the wiki accordingly. Fixed the setup where r2py was called before getting installed. Big change in the mapping. From now on the STAR index will be done without a GTF file. This allows to change the overhang option on the fly for each sample based on the mean read length. This also opens up 2-pass mapping. You will have to regenerate your index for it to work. Changed generate_meta in order to fit the new STAR index without a GTF. You now have to give the path to the GTF file in the config.yaml","title":"Changed"},{"location":"CHANGELOG/#added_3","text":"min_count_per_umi in the config.yaml to decide how many times a Gene - UMI has to be found to be counted as one.","title":"Added"},{"location":"CHANGELOG/#023","text":"","title":"[0.23]"},{"location":"CHANGELOG/#changed_5","text":"pre_align steps will output a fastq.gz instead of a fastq file. fastqc.R is now compatible with paired and single end data. Changed a few options in GLOBAL for UMI and Cell_barcodes options. Now possible to change filtering settings. See WIKI STAR logs have been stripped of the STAR string. This is to allow for better compatibility with multiqc Removed fastqc folder and moved items to logs folder. Grouping all logs files for better multiqc compatibility. Changed generate_meta to generate-meta for keeping similar syntax between modes. Added seperate log files for stats and summary in the DetectBeadSynthesisErrors. Moved part of the README to the wiki. Changed the name of the first expression matrix extracted before the species plot to unfiltered_expression.","title":"Changed"},{"location":"CHANGELOG/#added_4","text":"You can now run Bulk Single or paired end RNAseq data. Started a wiki with a FAQ Added options in GLOBAL config.yaml. You can now choose a range of options for UMI and Barcode filtering. please refer to the wiki for more information. Support for MultiQC . MultiQC is a great way of summarising all of the logs from your experiment. As of today it supports 46 different modules (such as fastqc, trimmomatic, STAR, etc...) The generate-plots mode now produces a multiqc_report.html file in the plots folder. New plot! BCDrop.pdf is a new plot showing you how many barcode and UMIs you dropped from the raw data before aligning. This helps to track how many samples you might loose because of low quality reads in the barcoding.","title":"Added"},{"location":"CHANGELOG/#022","text":"","title":"[0.22]"},{"location":"CHANGELOG/#changed_6","text":"all subprocess.call replaced by shell from snakemake STAR aligner now not limited to 8 cores or threads but will use the maximum number provided in the local.yaml file Name from dropSeqPip to dropSeqPipe Fixed a bug where all stage1 steps used the same summary file. Now BC tagging, UMI tagging, starting trim and polyA trim have different summary files extract-expression now merges all the samples final count matrix into one per run (folder) Fixed a bug where the amount of total reads on the knee-plot was overinflated. Changed knee-plot mode to generate-plots .","title":"Changed"},{"location":"CHANGELOG/#added_5","text":"Temp files have been added in the pipeline. You can turn this off by using the --notemp option fastqc mode now available. Generates fastqc reports plus summary plots Summary file and plot for fastqc and STAR logs Missing R packages should install automatically now. No need to install them beforehand. Report any problem plz GLOBAL values in the config files are now available. They allow to change UMI and BC ranges as well as mismatches for STAR aligner Added a new mode: generate_meta. This allows to create all the metadata files needed for the pipeline. You just need a folder with a genome.fa and an annotation.gtf","title":"Added"},{"location":"CHANGELOG/#021","text":"","title":"[0.21]"},{"location":"CHANGELOG/#added_6","text":"Changelog file to track changes --rerun option to force a rerun Multiple steps allowd now","title":"Added"},{"location":"CHANGELOG/#02-2017-03-14","text":"","title":"[0.2] - 2017-03-14"},{"location":"CHANGELOG/#changed_7","text":"The pipeline is now a python package being called as an executable Went from json to yaml for config files","title":"Changed"},{"location":"CHANGELOG/#added_7","text":"setup.py and dependencies Species plot available","title":"Added"},{"location":"CHANGELOG/#removed_3","text":"primer handling, went to default: AAGCAGTGGTATCAACGCAGAGTAC","title":"Removed"},{"location":"CHANGELOG/#01-2017-02-13","text":"","title":"[0.1] - 2017-02-13"},{"location":"CHANGELOG/#first-release","text":"Allows for preprocessing, alignement with STAR, post align processing until knee-plot","title":"First release"},{"location":"Create-config-files/","text":"Config file and sample file In order to run the pipeline you will need to complete the config.yaml file and the samples.csv file. Both are located in the templates folder and should be moved to the root folder of the experiment. 1. config.yaml - Executables, system and experiment parameters The config.yaml contains the paths to the the Drop-Seq tools wrapper, paths to references and parameters for each step of the pipeline. :*emp-directory: /path/to/temp/or/scratch/folder r*q-wrapper: /path/to/drop-seq-tools-wrapper.sh memory: 4g META: *cies: * - SPECIES_ONE * - SPECIES_TWO ratio: 0.2 reference-file: reference.fasta annotation-file: annotation.gtf reference-directory: /path/references/files/ FILTER: 5-prime-smart-adapter: DEPENDS ON THE PROTOCOL cell-barcode: start: end: min-quality: num-below-quality: UMI-barcode: start: end: min-quality: num-below-quality: trimmomatic: adapters-file: /path/to/adapters.fa LEADING: 3 TRAILING: 3 SLIDINGWINDOW: windowSize: 4 requiredQuality: 20 MINLEN: 20 ILLUMINACLIP: seedMismatches: 2 palindromeClipThreshold: 30 simpleClipThreshold: 10 MAPPING: STAR: outFilterMismatchNmax: 10 outFilterMismatchNoverLmax: 0.3 outFilterMismatchNoverReadLmax: 1 outFilterMatchNmin: 0 outFilterMatchNminOverLread: 0.66 outFilterScoreMinOverLread: 0.66 EXTRACTION: UMI-edit-distance: minimum-counts-per-UMI: Please note the \"space\" after the colon, is needed for the yaml to work. Subsections [LOCAL] temp-directory is the temp or scratch folder with enough space to keep temporary files. dropseq-wrapper is the wrapper that will call all the drop-seq-tools executables. memory is the maximum memory allocation pool for a Java Virtual Machine. [META] species is the species used in the experiment. [Required for mixed species experiment] Ex: species: - MOUSE - HUMAN ratio is how much \"contamination\" from another species you allow to validate them as a species or mixed. 0.2 means you allow a maximum of 20% mixing. Note: Those species name must reflect names used in the genome.fasta reference-file is the reference file of your genome (fasta). annotation-file is the gene annotation file (GTF). reference-folder is the folder where both of these files are gonna be stored. [FILTER] 5-prime-smart-adapter is the 5\" smart adapter used in your protocol. cell-barcode and UMI-barcode : Is the section for cell/umi barcode filtering. start is the first base position of your cell/umi barcode. end is the last base position of your cell/umi barcode. min-quality is the minimum quality filtering for bases in your cell barcodes or UMI to discard reads. num-below-quality is the maximum bases under min_quality . A value of 0 means all reads that have more than 0 (1 or more) bases under min-quality in the cell/umi barcode are discarded. trimmomatic : Is the section for trimming. adapters-file is the file containing your list of adapters as fasta. you can choose between 6 files in the templates folder, add any sequence to existing files or provide your own custom one. NexteraPE-PE.fa TruSeq2-PE.fa TruSeq2-SE.fa TruSeq3-PE-2.fa TruSeq3-PE.fa TruSeq3-SE.fa Provide the path to the file you want to use for trimming. If you want to add custom sequences or create a complete new one, I would advise to store it in the ROOT folder of the experiment. This will ensure that your custom file will not be overwritten if you update the pipeline. Example: NexteraPE-PE.fa LEADING (default:3) quality: Specifies the minimum quality required to keep a base. TRAILING (default:3) quality: Specifies the minimum quality required to keep a base. SLIDINGWINDOW : windowSize (default:4) specifies the number of bases to average across. requiredQuality (default:20) specifies the average quality required. MINLEN (default:20) Specifies the minimum length of reads to be kept. ILLUMINACLIP : seedMismatches (default:2) specifies the maximum mismatch count which will still allow a full match to be performed. palindromeClipThreshold (default:30) specifies how accurate the match between the two 'adapter ligated' reads must be for PE palindrome read alignment. simpleClipThreshold (default:10) specifies how accurate the match between any adapter etc. sequence must be against a read. All of the values for trimmomatic are the default ones. For details about trimmomatic parameters and what they do, please refer to the trimmomatic web page . [MAPPING] STAR outFilterMismatchNmax (default:10) is the maximum number of mismatches allowed. outFilterMismatchNoverLmax (default:0.3) is the maximum ratio of mismatched bases that mapped. outFilterMismatchNoverReadLmax (default:1.0) is the maximum ratio of mismatched bases of the whole read. outFilterMatchNmin (default:0) is the minimum number of matched bases. outFilterMatchNminOverLread (default:0.66) alignment will be output only if the ratio of matched bases is higher than or equal to this value. outFilterScoreMinOverLread (default:0.66) alignment will be output only if its ratio score is higher than or equal to this value. All of the values for STAR are the default ones. For details about STAR parameters and what they do, please refer to the STAR manual on git . [EXTRACTION] UMI-edit-distance This is the maximum manhattan distance between two UMI barcode when extracting count matrices. min-count-per-umi is the minimum UMI/Gene pair needed to be counted as one. 2. samples.csv - Samples parameters This file holds the sample names, expected cell numbers and read length for each sample. The file has to have this format: samples,expected_cells,read_lengths,batch sample_name1,500,100,Batch1 sample_name2,500,100,Batch2 expected_cells is the amount of cells you expect from your sample. read_length is the read length of the mRNA (Read2). This is necessary for STAR index generation batch is the batch of your sample. If you are added new samples to the same experiment, this is typically a good place to add the main batch. Note: You can add any other column you wish here, it won't affect the pipeline and you can use it later on in your analysis. Next step downloading the reference files","title":"Config file and sample file"},{"location":"Create-config-files/#config-file-and-sample-file","text":"In order to run the pipeline you will need to complete the config.yaml file and the samples.csv file. Both are located in the templates folder and should be moved to the root folder of the experiment.","title":"Config file and sample file"},{"location":"Create-config-files/#1-configyaml-executables-system-and-experiment-parameters","text":"The config.yaml contains the paths to the the Drop-Seq tools wrapper, paths to references and parameters for each step of the pipeline. :*emp-directory: /path/to/temp/or/scratch/folder r*q-wrapper: /path/to/drop-seq-tools-wrapper.sh memory: 4g META: *cies: * - SPECIES_ONE * - SPECIES_TWO ratio: 0.2 reference-file: reference.fasta annotation-file: annotation.gtf reference-directory: /path/references/files/ FILTER: 5-prime-smart-adapter: DEPENDS ON THE PROTOCOL cell-barcode: start: end: min-quality: num-below-quality: UMI-barcode: start: end: min-quality: num-below-quality: trimmomatic: adapters-file: /path/to/adapters.fa LEADING: 3 TRAILING: 3 SLIDINGWINDOW: windowSize: 4 requiredQuality: 20 MINLEN: 20 ILLUMINACLIP: seedMismatches: 2 palindromeClipThreshold: 30 simpleClipThreshold: 10 MAPPING: STAR: outFilterMismatchNmax: 10 outFilterMismatchNoverLmax: 0.3 outFilterMismatchNoverReadLmax: 1 outFilterMatchNmin: 0 outFilterMatchNminOverLread: 0.66 outFilterScoreMinOverLread: 0.66 EXTRACTION: UMI-edit-distance: minimum-counts-per-UMI: Please note the \"space\" after the colon, is needed for the yaml to work.","title":"1. config.yaml - Executables, system and experiment parameters"},{"location":"Create-config-files/#subsections","text":"","title":"Subsections"},{"location":"Create-config-files/#local","text":"temp-directory is the temp or scratch folder with enough space to keep temporary files. dropseq-wrapper is the wrapper that will call all the drop-seq-tools executables. memory is the maximum memory allocation pool for a Java Virtual Machine.","title":"[LOCAL]"},{"location":"Create-config-files/#meta","text":"species is the species used in the experiment. [Required for mixed species experiment] Ex: species: - MOUSE - HUMAN ratio is how much \"contamination\" from another species you allow to validate them as a species or mixed. 0.2 means you allow a maximum of 20% mixing. Note: Those species name must reflect names used in the genome.fasta reference-file is the reference file of your genome (fasta). annotation-file is the gene annotation file (GTF). reference-folder is the folder where both of these files are gonna be stored.","title":"[META]"},{"location":"Create-config-files/#filter","text":"5-prime-smart-adapter is the 5\" smart adapter used in your protocol. cell-barcode and UMI-barcode : Is the section for cell/umi barcode filtering. start is the first base position of your cell/umi barcode. end is the last base position of your cell/umi barcode. min-quality is the minimum quality filtering for bases in your cell barcodes or UMI to discard reads. num-below-quality is the maximum bases under min_quality . A value of 0 means all reads that have more than 0 (1 or more) bases under min-quality in the cell/umi barcode are discarded. trimmomatic : Is the section for trimming. adapters-file is the file containing your list of adapters as fasta. you can choose between 6 files in the templates folder, add any sequence to existing files or provide your own custom one. NexteraPE-PE.fa TruSeq2-PE.fa TruSeq2-SE.fa TruSeq3-PE-2.fa TruSeq3-PE.fa TruSeq3-SE.fa Provide the path to the file you want to use for trimming. If you want to add custom sequences or create a complete new one, I would advise to store it in the ROOT folder of the experiment. This will ensure that your custom file will not be overwritten if you update the pipeline. Example: NexteraPE-PE.fa LEADING (default:3) quality: Specifies the minimum quality required to keep a base. TRAILING (default:3) quality: Specifies the minimum quality required to keep a base. SLIDINGWINDOW : windowSize (default:4) specifies the number of bases to average across. requiredQuality (default:20) specifies the average quality required. MINLEN (default:20) Specifies the minimum length of reads to be kept. ILLUMINACLIP : seedMismatches (default:2) specifies the maximum mismatch count which will still allow a full match to be performed. palindromeClipThreshold (default:30) specifies how accurate the match between the two 'adapter ligated' reads must be for PE palindrome read alignment. simpleClipThreshold (default:10) specifies how accurate the match between any adapter etc. sequence must be against a read. All of the values for trimmomatic are the default ones. For details about trimmomatic parameters and what they do, please refer to the trimmomatic web page .","title":"[FILTER]"},{"location":"Create-config-files/#mapping","text":"STAR outFilterMismatchNmax (default:10) is the maximum number of mismatches allowed. outFilterMismatchNoverLmax (default:0.3) is the maximum ratio of mismatched bases that mapped. outFilterMismatchNoverReadLmax (default:1.0) is the maximum ratio of mismatched bases of the whole read. outFilterMatchNmin (default:0) is the minimum number of matched bases. outFilterMatchNminOverLread (default:0.66) alignment will be output only if the ratio of matched bases is higher than or equal to this value. outFilterScoreMinOverLread (default:0.66) alignment will be output only if its ratio score is higher than or equal to this value. All of the values for STAR are the default ones. For details about STAR parameters and what they do, please refer to the STAR manual on git .","title":"[MAPPING]"},{"location":"Create-config-files/#extraction","text":"UMI-edit-distance This is the maximum manhattan distance between two UMI barcode when extracting count matrices. min-count-per-umi is the minimum UMI/Gene pair needed to be counted as one.","title":"[EXTRACTION]"},{"location":"Create-config-files/#2-samplescsv-samples-parameters","text":"This file holds the sample names, expected cell numbers and read length for each sample. The file has to have this format: samples,expected_cells,read_lengths,batch sample_name1,500,100,Batch1 sample_name2,500,100,Batch2 expected_cells is the amount of cells you expect from your sample. read_length is the read length of the mRNA (Read2). This is necessary for STAR index generation batch is the batch of your sample. If you are added new samples to the same experiment, this is typically a good place to add the main batch. Note: You can add any other column you wish here, it won't affect the pipeline and you can use it later on in your analysis. Next step downloading the reference files","title":"2. samples.csv - Samples parameters"},{"location":"FAQ/","text":"FAQ 1. I get error='Cannot allocate memory' (errno=12) , what should I do. [Fixed] This has been fixed by using a wrapper exposing the TMPDIR to the pipeline. First, be sure that your TMPDIR from the first configuration yaml has at least 100Go. If you still have problems, you should edit the following files in the Drop-seq_tools-1.12: TagBamWithReadSequenceExtended FilterBAM TrimStartingSequence PolyATrimmer TagReadWithGeneExon DetectBeadSynthesisErrors SingleCellRnaSeqMetricsCollector BAMTagHistogram In each of those files, the last line should be something like: java -Xmx${xmx} -Djava.io.tmpdir=/path/to/temp/folder/ -jar $jar_deploy_dir/dropseq.jar $progname $* You can also use this simple bash script to do it: Replace /path/to/temp/folder/ with your temp path and don't forget to use escapes for / for f in BAMTagHistogram SingleCellRnaSeqMetricsCollector DetectBeadSynthesisErrors TagReadWithGeneExon PolyATrimmer TrimStartingSequence FilterBAM TagBamWithReadSequenceExtended do sed -i 's/java -Xmx${xmx}/java -Xmx${xmx} -Djava.io.tmpdir=/path/to/temp/folder/ /g' $f done","title":"FAQ"},{"location":"FAQ/#faq","text":"","title":"FAQ"},{"location":"FAQ/#1-i-get-errorcannot-allocate-memory-errno12-what-should-i-do-fixed","text":"This has been fixed by using a wrapper exposing the TMPDIR to the pipeline. First, be sure that your TMPDIR from the first configuration yaml has at least 100Go. If you still have problems, you should edit the following files in the Drop-seq_tools-1.12: TagBamWithReadSequenceExtended FilterBAM TrimStartingSequence PolyATrimmer TagReadWithGeneExon DetectBeadSynthesisErrors SingleCellRnaSeqMetricsCollector BAMTagHistogram In each of those files, the last line should be something like: java -Xmx${xmx} -Djava.io.tmpdir=/path/to/temp/folder/ -jar $jar_deploy_dir/dropseq.jar $progname $* You can also use this simple bash script to do it: Replace /path/to/temp/folder/ with your temp path and don't forget to use escapes for / for f in BAMTagHistogram SingleCellRnaSeqMetricsCollector DetectBeadSynthesisErrors TagReadWithGeneExon PolyATrimmer TrimStartingSequence FilterBAM TagBamWithReadSequenceExtended do sed -i 's/java -Xmx${xmx}/java -Xmx${xmx} -Djava.io.tmpdir=/path/to/temp/folder/ /g' $f done","title":"1. I get error='Cannot allocate memory' (errno=12), what should I do. [Fixed]"},{"location":"Home/","text":"Welcome You will find here all the information you need to run the pipeline 0.31 . If you still having questions after having read all the wiki, don't hesitate to post an issue","title":"Home"},{"location":"Home/#welcome","text":"You will find here all the information you need to run the pipeline 0.31 . If you still having questions after having read all the wiki, don't hesitate to post an issue","title":"Welcome"},{"location":"Installation/","text":"This pipeline is dependent on conda. Step 1: Download and install miniconda3 First you need to download and install miniconda3: for linux wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh for mac os curl https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh -o Miniconda3-latest-MacOSX-x86_64.sh bash Miniconda3-latest-MacOSX-x86_64.sh Step 2: Clone the workflow Clone the worflow git clone https://github.com/Hoohm/dropSeqPipe.git Step 3: Install snakemake conda install -c bioconda -c conda-forge snakemake Next step is config files completion Complete the config.yaml with the missing information UPDATES: How to update the pipeline Go to your experiment folder, then pull. git pull https://github.com/Hoohm/dropSeqPipe.git If you want to update files/plots based on the updates you can use this command: snakemake -R `snakemake --list-codes-changes` This will update all the files that would be modified by the changes in the code (rules or script). Depending on how much and where the changes have been made, this might rerun the whole pipeline.","title":"Installation"},{"location":"Installation/#step-1-download-and-install-miniconda3","text":"First you need to download and install miniconda3: for linux wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh for mac os curl https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh -o Miniconda3-latest-MacOSX-x86_64.sh bash Miniconda3-latest-MacOSX-x86_64.sh","title":"Step 1: Download and install miniconda3"},{"location":"Installation/#step-2-clone-the-workflow","text":"Clone the worflow git clone https://github.com/Hoohm/dropSeqPipe.git","title":"Step 2: Clone the workflow"},{"location":"Installation/#step-3-install-snakemake","text":"conda install -c bioconda -c conda-forge snakemake Next step is config files completion Complete the config.yaml with the missing information","title":"Step 3: Install snakemake"},{"location":"Installation/#updates-how-to-update-the-pipeline","text":"Go to your experiment folder, then pull. git pull https://github.com/Hoohm/dropSeqPipe.git If you want to update files/plots based on the updates you can use this command: snakemake -R `snakemake --list-codes-changes` This will update all the files that would be modified by the changes in the code (rules or script). Depending on how much and where the changes have been made, this might rerun the whole pipeline.","title":"UPDATES: How to update the pipeline"},{"location":"Plots/","text":"On of the main purpose of this package is getting information about your data to improve your protocol and filter your data for further downstream analysis. Here is a list of plots and reports that you will get from the pipeline. Fastqc, STAR and trimmomatic reports are now generated as multiqc reports in the reports folder. 1. Knee plot (per sample) On the x axis is the cumulative fraction of reads per STAMPS (captured cell). On the y axis is the ordered STAMPS (based on total reads). This allows you to determine how much of the reads you actually captured with the number of cells you expected. The cutting is based on the expected_cells parameter in the samples.csv file. If you see a clear bend on the plot that is higher in the number of cells than what you expected, you should increase the expected_cells value and rerun the extract step. If it is under, I would advise to filter out your data with a downstream analysis tool such as Seurat. Note: I advise not to try to discover \"real\" cells/STAMPS at this stage. I suggest to extract the expected number of cells and filter out later in post-processing with other kind of meta data. 2. Cell barcode Quality trim (per sample) On the x axis are the number of failed bases in the cell barcodes. On the y axis are the number of cell barcodes. This plot allows you to adjust your filter of minimum quality and number of allowed minimum quality for the filtering process of the cell barcodes. 3. UMI Quality trim (per sample) On the x axis are the number of failed bases in the UMI barcode. On the y axis are the number of UMI barcodes. This plot allows you to adjust your filter of minimum quality and number of allowed minimum quality for the filtering process of the UMIs. 4. PolyA trimming of reads (per sample) On the x axis are the length of the trimmed polyA On the y axis are the number of trimmed polyA. This plot shows you the distribution of the polyA trimming step. 5. SMART adapter trimming of reads (per sample) On the x axis are the length of the trimmed SMART adapter On the y axis are the number of trimmed SMART adapter. This plot shows you the distribution of the SMART adapter trimming step. 6. RNA metrics (per sample) On the x axis are top barcodes based on your expected_cells values or the barcodes.csv file. Top plot: On the y axis are the number of bases classified by region of mapping. Bottom plot: On the y axis are the percentage of bases classified by region of mapping. This plot gives a lot of different informations. The top plot allows you to quickly compare cells between them in terms of how much has been mapped. This can sometimes help identify outliers or bad runs. The bottom plot allows you to find cells that have an \"abnormal\" mapped base distribution compared to other cells. 7. Cell and UMI dropped Barcodes (across samples) On the x axis are the samples. TOP: On the y axis are the number of reads discarded. BOTTOM: On the y axis are the percentage of reads discarded. This plot allows you to adjust your filter of minimum quality and number of allowed minimum quality for the filtering process of the UMIs. The Not_dropped category is going to be mapped in STAR. 8. Yield (across samples) On the x axis are the samples. TOP: On the y axis are the number of reads attributed to each category. BOTTOM: On the y axis are the percentage of attributed to each category. This plot gives you an overview of all the reads from your samples and how they are distributed in all the possible categories. The reads that are uniquely mapped ar the ones you will keep at the end for the UMI count matrix. Mixed experiment 8. Barnyard plot (per sample) This plot shows you species purity for each STAMPS. Mixed and No call STAMPS are dropped and only single species are kept for extraction. You can change the minimum ratio of transcripts to define a STAMP as mixed or not in the configfile with: species_ratio You get one plot for genes and one plot for transcripts. The selection is done on the transcript level.","title":"Plots"},{"location":"Plots/#1-knee-plot-per-sample","text":"On the x axis is the cumulative fraction of reads per STAMPS (captured cell). On the y axis is the ordered STAMPS (based on total reads). This allows you to determine how much of the reads you actually captured with the number of cells you expected. The cutting is based on the expected_cells parameter in the samples.csv file. If you see a clear bend on the plot that is higher in the number of cells than what you expected, you should increase the expected_cells value and rerun the extract step. If it is under, I would advise to filter out your data with a downstream analysis tool such as Seurat. Note: I advise not to try to discover \"real\" cells/STAMPS at this stage. I suggest to extract the expected number of cells and filter out later in post-processing with other kind of meta data.","title":"1. Knee plot (per sample)"},{"location":"Plots/#2-cell-barcode-quality-trim-per-sample","text":"On the x axis are the number of failed bases in the cell barcodes. On the y axis are the number of cell barcodes. This plot allows you to adjust your filter of minimum quality and number of allowed minimum quality for the filtering process of the cell barcodes.","title":"2. Cell barcode Quality trim (per sample)"},{"location":"Plots/#3-umi-quality-trim-per-sample","text":"On the x axis are the number of failed bases in the UMI barcode. On the y axis are the number of UMI barcodes. This plot allows you to adjust your filter of minimum quality and number of allowed minimum quality for the filtering process of the UMIs.","title":"3. UMI Quality trim (per sample)"},{"location":"Plots/#4-polya-trimming-of-reads-per-sample","text":"On the x axis are the length of the trimmed polyA On the y axis are the number of trimmed polyA. This plot shows you the distribution of the polyA trimming step.","title":"4. PolyA trimming of reads (per sample)"},{"location":"Plots/#5-smart-adapter-trimming-of-reads-per-sample","text":"On the x axis are the length of the trimmed SMART adapter On the y axis are the number of trimmed SMART adapter. This plot shows you the distribution of the SMART adapter trimming step.","title":"5. SMART adapter trimming of reads (per sample)"},{"location":"Plots/#6-rna-metrics-per-sample","text":"On the x axis are top barcodes based on your expected_cells values or the barcodes.csv file. Top plot: On the y axis are the number of bases classified by region of mapping. Bottom plot: On the y axis are the percentage of bases classified by region of mapping. This plot gives a lot of different informations. The top plot allows you to quickly compare cells between them in terms of how much has been mapped. This can sometimes help identify outliers or bad runs. The bottom plot allows you to find cells that have an \"abnormal\" mapped base distribution compared to other cells.","title":"6. RNA metrics (per sample)"},{"location":"Plots/#7-cell-and-umi-dropped-barcodes-across-samples","text":"On the x axis are the samples. TOP: On the y axis are the number of reads discarded. BOTTOM: On the y axis are the percentage of reads discarded. This plot allows you to adjust your filter of minimum quality and number of allowed minimum quality for the filtering process of the UMIs. The Not_dropped category is going to be mapped in STAR.","title":"7. Cell and UMI dropped Barcodes (across samples)"},{"location":"Plots/#8-yield-across-samples","text":"On the x axis are the samples. TOP: On the y axis are the number of reads attributed to each category. BOTTOM: On the y axis are the percentage of attributed to each category. This plot gives you an overview of all the reads from your samples and how they are distributed in all the possible categories. The reads that are uniquely mapped ar the ones you will keep at the end for the UMI count matrix.","title":"8. Yield (across samples)"},{"location":"Plots/#mixed-experiment","text":"","title":"Mixed experiment"},{"location":"Plots/#8-barnyard-plot-per-sample","text":"This plot shows you species purity for each STAMPS. Mixed and No call STAMPS are dropped and only single species are kept for extraction. You can change the minimum ratio of transcripts to define a STAMP as mixed or not in the configfile with: species_ratio You get one plot for genes and one plot for transcripts. The selection is done on the transcript level.","title":"8. Barnyard plot (per sample)"},{"location":"Reference-Files/","text":"Reference files generation Before running the pipeline you will need to download a reference genome as well as the GTF annotation. dropSeqPipe is based on multiple files derived from those two files. Those will be automatically generated when you run the pipeline. As an example, we use ensembl reference and annotation located here . The fasta reference is the DNA and the GTF is the gene sets. All you need to do is put the reference genome as well as the GTF file in a folder (extentions are crucial. it won't run otherwise): genome.fasta annotation.gtf Once the pipeline has run completely, the folder will look like this: genome.fa annotation.gtf annotation.refFlat annotation_reduced.gtf genome.consensus_introns.intervals genome.dict genome.exons.intervals genome.genes.intervals genome.intergenic.intervals genome.rRNA.intervals STAR_INDEX/SA_read_length/ Note: The STAR index will be built based on the read length of your mRNA read (Read2). If you have different lengths, it will produce multiple indexes. Finally, you can now run the pipeline","title":"Reference Files"},{"location":"Reference-Files/#reference-files-generation","text":"Before running the pipeline you will need to download a reference genome as well as the GTF annotation. dropSeqPipe is based on multiple files derived from those two files. Those will be automatically generated when you run the pipeline. As an example, we use ensembl reference and annotation located here . The fasta reference is the DNA and the GTF is the gene sets. All you need to do is put the reference genome as well as the GTF file in a folder (extentions are crucial. it won't run otherwise): genome.fasta annotation.gtf Once the pipeline has run completely, the folder will look like this: genome.fa annotation.gtf annotation.refFlat annotation_reduced.gtf genome.consensus_introns.intervals genome.dict genome.exons.intervals genome.genes.intervals genome.intergenic.intervals genome.rRNA.intervals STAR_INDEX/SA_read_length/ Note: The STAR index will be built based on the read length of your mRNA read (Read2). If you have different lengths, it will produce multiple indexes. Finally, you can now run the pipeline","title":"Reference files generation"},{"location":"Running-dropSeqPipe/","text":"Context The pipeline is supposed to be cloned once and then run on a folder containing your data as well as the configuration files. The run folder can contain multiple runs (aka batches) as you can easily add new samples when recieving new data and run the same commands. This will simply run the pipeline on the newly added data and recreate reports as well as plots containing all the samples. Example: You run 2 biological conditions with 2 replicates. This makes up for 4 samples. Assume a simple dropseq protocol with only human cells. 1. You sequence the data and recieve the 8 files (two files per sample) and set up the pipeline 2. You run the pipeline with the command: snakemake --use-conda --cores N --directory WORKING_DIR N being the number of cores available and WORKING_DIR being the folder containing your data. 3. You see that there is an issue with the protocol and you modify it 4. You create a new set of libraries and sequence them (same 2x2 design) 5. You add the new files in the data folder of WORKING_DIR and edit the samples.csv to add missing samples. 6. You run the pipeline as you did the first time snakemake --use-conda --cores N --directory WORKING_DIR 7. This will run the new samples only and recreate the reports as well as the BC_drop and yield plots. 8. It is now easy to compare the impact of your change in the procotol Working dir folder preparation The raw data from the sequencer should be stored in the data folder of WORKING_DIR folder like this: /path/to/your/WORKING_DIR/ | -- data/ | -- -- sample1_R1.fastq.gz | -- -- sample1_R2.fastq.gz | samples.csv | config.yaml | barcodes.csv Note: In DropSeq or ScrbSeq you expect a paired sequencing. R1 will hold the information of your barcode and UMI, R2 will hold the 3' end of the captured mRNA. Once everything is in place, you can run the pipeline using the normal snakemake commands. Running the pipeline (TLDR version) For a simple single cell run you only need to run: snakemake --cores N --use-conda --directory WORKING_DIR This will run the whole pipeline and use the X number of cores you gave to it. Running the pipeline I highly recommend to take a look at the options that are available since I won't cover everything here. Modes You have two main ways to run the pipeline. You can either just run snakemake --use-conda --directory WORKING_DIR in the root folder containing your experiment and it will run everything without stopping. You can also run each step separately. The main advantage of the second way is that you are able to fine tune your parameters based on the results of fastqc, filtering, mapping quality, etc... I would suggest using the second approach when you work on a new protocol and the first one when you are confident of your parameters. There are seven different modes available and to run one specifically you need to call the mode. Example: To run the qc mode: snakemake --cores 8 qc --use-conda --directory WORKING_DIR You can also run multiple modes at the same time if you want: snakemake --cores 8 qc filter --use-conda --directory WORKING_DIR Single species: meta : Generates all the subsequent references files and STAR index needed to run the pipeline. You can run this alone if you just want to create the meta-data file before running a new set of data. qc : Creates fastqc reports of your data. filter : Go from sample_R1.fastq.gz to sample_filtered.fastq.gz ready to be mapped to the genome. This step filters out your data for low quality reads and trims adapter you provided in the FILTER section. map : Go from sample_filtered.fastq.gz to the sample_final.bam read to extract the expression data. This maps the data to the genom. extract : Extract the expression data. You'll get a umi and a count expression matrix from your whole experiment. Mixed species The next two step are only necessary for mixed experiment. Note that you can't just run snakemake --directory WORKING_DIR --use-conda for a mixed experiment but you have to run snakemake extract-species --directory WORKING_DIR instead. split_species : Poduces the species plot and the species specific barcode list for extract-species. extract_species : Extract the expression data. You'll get a umi and a count expression matrix from your whole experiment/specie in the summary folder. If you want all the plots for mixed experiment, you have to call all the modes: snakemake --cores 8 qc filter map split_species extract_species --use-conda --directory WORKING_DIR Barcode whitelist In protocols such as SCRBseq, the expected barcodes sequences are known. This pipeline also does allow the use of known barcodes instread of a number of expected cells. In order to use this functionnality you just need to add a barcodes.csv file at the root of the working dir and run the pipeline as usual. Advanced options If you have some specific adapters that are not present by default in the ones in the templates folder, you can add whatever adapters you want to trim (as many as you need) following the fasta syntax in any fasta files proposed. Be sure to specify it properly in the config.yaml at FILTER:IlluminaClip Further options --cores N Use this argument to use X amunt of cores available. --notemp Use this to not delete all the temporary files. Without this option, only files between steps are kept. Use this option if you are troobleshooting the pipeline or you want to analyze in between files yourself. --dryrun Use this to check out what is going to run if you run your command. This is nice to check for potential missing files. Running on clusters There is a file in the templates called cluster.yaml . This can be used to modify ressources needed for your data. I generally recommand moving the file to the root of the folder so that it doesn't get replaced by updates. Bellow is an example of running on a cluster using the template file cluster.yaml on SLURM. snakemake --cluster 'sbatch -n {cluster.n} -t {cluster.time} --clusters=CLUSTERNAME --output={cluster.output}' --jobs N --cluster-config cluster.yaml --use-conda --local-cores C N: is the number of jobs you are allowed to run at the same time C: is the local-cores of the host machine. A few simple rules are gonna be run locally (not sent to nodes) because they are not that heavy (mostly plotting) CLUSTERNAME: the name of the cluster you want to use Folder Structure This is the folder structure you get in the end: /path/to/your/WORKING_DIR/ | -- data/ | -- logs/ | -- -- cluster/ | -- plots/ | -- reports/ | -- summary/ | Snakefile | samples.csv | config.yaml | barcodes.csv | .snakemake/ data/ Contains all your samples as well as the intermediary files logs/ Contains all the logfiles generated by the pipeline logs/cluster Contains all the logfiles generated by the cluster plots/ Contains all the plots generated by the pipeline reports/ Contains all the reports generated by the pipeline summary/ Contains all the files you might use for downstream analysis (contains barcodes selected per sample per species, final umi/counts expression matrix) samples.csv File containing sample details config.yaml File containing pipeline parameters as well as system parameters .snakemake/ Folder that contains all the environements created for the run as well as a lot of other things that I don't know about.","title":"Running dropSeqPipe"},{"location":"Running-dropSeqPipe/#context","text":"The pipeline is supposed to be cloned once and then run on a folder containing your data as well as the configuration files. The run folder can contain multiple runs (aka batches) as you can easily add new samples when recieving new data and run the same commands. This will simply run the pipeline on the newly added data and recreate reports as well as plots containing all the samples. Example: You run 2 biological conditions with 2 replicates. This makes up for 4 samples. Assume a simple dropseq protocol with only human cells. 1. You sequence the data and recieve the 8 files (two files per sample) and set up the pipeline 2. You run the pipeline with the command: snakemake --use-conda --cores N --directory WORKING_DIR N being the number of cores available and WORKING_DIR being the folder containing your data. 3. You see that there is an issue with the protocol and you modify it 4. You create a new set of libraries and sequence them (same 2x2 design) 5. You add the new files in the data folder of WORKING_DIR and edit the samples.csv to add missing samples. 6. You run the pipeline as you did the first time snakemake --use-conda --cores N --directory WORKING_DIR 7. This will run the new samples only and recreate the reports as well as the BC_drop and yield plots. 8. It is now easy to compare the impact of your change in the procotol","title":"Context"},{"location":"Running-dropSeqPipe/#working-dir-folder-preparation","text":"The raw data from the sequencer should be stored in the data folder of WORKING_DIR folder like this: /path/to/your/WORKING_DIR/ | -- data/ | -- -- sample1_R1.fastq.gz | -- -- sample1_R2.fastq.gz | samples.csv | config.yaml | barcodes.csv Note: In DropSeq or ScrbSeq you expect a paired sequencing. R1 will hold the information of your barcode and UMI, R2 will hold the 3' end of the captured mRNA. Once everything is in place, you can run the pipeline using the normal snakemake commands.","title":"Working dir folder preparation"},{"location":"Running-dropSeqPipe/#running-the-pipeline-tldr-version","text":"For a simple single cell run you only need to run: snakemake --cores N --use-conda --directory WORKING_DIR This will run the whole pipeline and use the X number of cores you gave to it.","title":"Running the pipeline (TLDR version)"},{"location":"Running-dropSeqPipe/#running-the-pipeline","text":"I highly recommend to take a look at the options that are available since I won't cover everything here.","title":"Running the pipeline"},{"location":"Running-dropSeqPipe/#modes","text":"You have two main ways to run the pipeline. You can either just run snakemake --use-conda --directory WORKING_DIR in the root folder containing your experiment and it will run everything without stopping. You can also run each step separately. The main advantage of the second way is that you are able to fine tune your parameters based on the results of fastqc, filtering, mapping quality, etc... I would suggest using the second approach when you work on a new protocol and the first one when you are confident of your parameters. There are seven different modes available and to run one specifically you need to call the mode. Example: To run the qc mode: snakemake --cores 8 qc --use-conda --directory WORKING_DIR You can also run multiple modes at the same time if you want: snakemake --cores 8 qc filter --use-conda --directory WORKING_DIR","title":"Modes"},{"location":"Running-dropSeqPipe/#single-species","text":"meta : Generates all the subsequent references files and STAR index needed to run the pipeline. You can run this alone if you just want to create the meta-data file before running a new set of data. qc : Creates fastqc reports of your data. filter : Go from sample_R1.fastq.gz to sample_filtered.fastq.gz ready to be mapped to the genome. This step filters out your data for low quality reads and trims adapter you provided in the FILTER section. map : Go from sample_filtered.fastq.gz to the sample_final.bam read to extract the expression data. This maps the data to the genom. extract : Extract the expression data. You'll get a umi and a count expression matrix from your whole experiment.","title":"Single species:"},{"location":"Running-dropSeqPipe/#mixed-species","text":"The next two step are only necessary for mixed experiment. Note that you can't just run snakemake --directory WORKING_DIR --use-conda for a mixed experiment but you have to run snakemake extract-species --directory WORKING_DIR instead. split_species : Poduces the species plot and the species specific barcode list for extract-species. extract_species : Extract the expression data. You'll get a umi and a count expression matrix from your whole experiment/specie in the summary folder. If you want all the plots for mixed experiment, you have to call all the modes: snakemake --cores 8 qc filter map split_species extract_species --use-conda --directory WORKING_DIR","title":"Mixed species"},{"location":"Running-dropSeqPipe/#barcode-whitelist","text":"In protocols such as SCRBseq, the expected barcodes sequences are known. This pipeline also does allow the use of known barcodes instread of a number of expected cells. In order to use this functionnality you just need to add a barcodes.csv file at the root of the working dir and run the pipeline as usual.","title":"Barcode whitelist"},{"location":"Running-dropSeqPipe/#advanced-options","text":"If you have some specific adapters that are not present by default in the ones in the templates folder, you can add whatever adapters you want to trim (as many as you need) following the fasta syntax in any fasta files proposed. Be sure to specify it properly in the config.yaml at FILTER:IlluminaClip","title":"Advanced options"},{"location":"Running-dropSeqPipe/#further-options","text":"--cores N Use this argument to use X amunt of cores available. --notemp Use this to not delete all the temporary files. Without this option, only files between steps are kept. Use this option if you are troobleshooting the pipeline or you want to analyze in between files yourself. --dryrun Use this to check out what is going to run if you run your command. This is nice to check for potential missing files.","title":"Further options"},{"location":"Running-dropSeqPipe/#running-on-clusters","text":"There is a file in the templates called cluster.yaml . This can be used to modify ressources needed for your data. I generally recommand moving the file to the root of the folder so that it doesn't get replaced by updates. Bellow is an example of running on a cluster using the template file cluster.yaml on SLURM. snakemake --cluster 'sbatch -n {cluster.n} -t {cluster.time} --clusters=CLUSTERNAME --output={cluster.output}' --jobs N --cluster-config cluster.yaml --use-conda --local-cores C N: is the number of jobs you are allowed to run at the same time C: is the local-cores of the host machine. A few simple rules are gonna be run locally (not sent to nodes) because they are not that heavy (mostly plotting) CLUSTERNAME: the name of the cluster you want to use","title":"Running on clusters"},{"location":"Running-dropSeqPipe/#folder-structure","text":"This is the folder structure you get in the end: /path/to/your/WORKING_DIR/ | -- data/ | -- logs/ | -- -- cluster/ | -- plots/ | -- reports/ | -- summary/ | Snakefile | samples.csv | config.yaml | barcodes.csv | .snakemake/ data/ Contains all your samples as well as the intermediary files logs/ Contains all the logfiles generated by the pipeline logs/cluster Contains all the logfiles generated by the cluster plots/ Contains all the plots generated by the pipeline reports/ Contains all the reports generated by the pipeline summary/ Contains all the files you might use for downstream analysis (contains barcodes selected per sample per species, final umi/counts expression matrix) samples.csv File containing sample details config.yaml File containing pipeline parameters as well as system parameters .snakemake/ Folder that contains all the environements created for the run as well as a lot of other things that I don't know about.","title":"Folder Structure"}]}